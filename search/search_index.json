{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About \u00b6 A Simple Stateless service which can DMux Source to Sink. This was Built for High Throughput Processing over KafkaStream Source and HttpEndpoint Sink. Application Overview \u00b6 Core of go-dmux is to take any source and connect to any sink. Dmux takes distributor and size as argument which enables it to distribute data from source to sink in a FanOut manner. KafkaSource and HttpSink have been added by default, as that was the major use case Dmux was built to solve for. Prior to Dmux we were using Storm and Spark for Stream processing applications. Dmux is a simpler and predictive alternative to Storm, Spark or other Stream processing systems. Dmux goal is have developer focus on writing app's and have go-dmux to redirect messages from Kafka to app's. This enables reuse of App monitoring infrastructure which is already in place and also in NFR tuning. The key difference between Dmux and Storm is much lesser number of hops and queues involved in Single Message processing. Dmux has single hop from Source to Sink. With reference to queueing theory , ability to estimate when a system will be queued up and degrade is simpler with Dmux (M/M/1). This gets fairly complicated with Storm which has two way queues per Spout, Bolt and OffsetAckers. Dmux helps in making it more simple to predict the number boxes needed to scale. Note Go-Dmux does not support the concepts around higher order processing primitives such as ParDo or Aggregate by time window. These are not primitives in Storm as well. As Trident is Built over Storm such constructs can be built over App Framework using Datastore.","title":"About"},{"location":"#about","text":"A Simple Stateless service which can DMux Source to Sink. This was Built for High Throughput Processing over KafkaStream Source and HttpEndpoint Sink.","title":"About"},{"location":"#application-overview","text":"Core of go-dmux is to take any source and connect to any sink. Dmux takes distributor and size as argument which enables it to distribute data from source to sink in a FanOut manner. KafkaSource and HttpSink have been added by default, as that was the major use case Dmux was built to solve for. Prior to Dmux we were using Storm and Spark for Stream processing applications. Dmux is a simpler and predictive alternative to Storm, Spark or other Stream processing systems. Dmux goal is have developer focus on writing app's and have go-dmux to redirect messages from Kafka to app's. This enables reuse of App monitoring infrastructure which is already in place and also in NFR tuning. The key difference between Dmux and Storm is much lesser number of hops and queues involved in Single Message processing. Dmux has single hop from Source to Sink. With reference to queueing theory , ability to estimate when a system will be queued up and degrade is simpler with Dmux (M/M/1). This gets fairly complicated with Storm which has two way queues per Spout, Bolt and OffsetAckers. Dmux helps in making it more simple to predict the number boxes needed to scale. Note Go-Dmux does not support the concepts around higher order processing primitives such as ParDo or Aggregate by time window. These are not primitives in Storm as well. As Trident is Built over Storm such constructs can be built over App Framework using Datastore.","title":"Application Overview"},{"location":"Architecture/","text":"Dmux Components \u00b6 DmuxConnection connects one Source to Sink. DmuxInstance can run multiple DmuxConnections. Multiple DmuxInstances running same DmuxConnection will load balance. LoadBalance is function of Source. In case of KafkaSource it load balances using Zookepeer. DmuxInstance is HA stateless component which you can install on any compute VM and scale out to the number of Kafka Partitions. Good to use Instance group to resize. DmuxConnection Block Digram \u00b6 DmuxConnection connects Source to Sink. Example connectionTypes = kafka_http, kafka_foxtrot. DmuxConnection enables you to configure dmuxSize concurrent consumer Sinks along with batching to achieve high Throughput. Dmux is written Go and leverages go coroutines and channels to achieve high vertical scale. Most cases would need just one Dmux instance to run many DmuxConnections. KafkaSource interface in Dmux is a High Available KafkaConsumer which reuses the Zookeeper used by KafkaBrokers for Partition Balancing and Offset management. During Partition Rebalancing between Dmux instances, Client can expect replay but there will be no data loss. Dmux ensures ordering per Key when distributor = Hash. Note this is default distributor type.","title":"Architecture"},{"location":"Architecture/#dmux-components","text":"DmuxConnection connects one Source to Sink. DmuxInstance can run multiple DmuxConnections. Multiple DmuxInstances running same DmuxConnection will load balance. LoadBalance is function of Source. In case of KafkaSource it load balances using Zookepeer. DmuxInstance is HA stateless component which you can install on any compute VM and scale out to the number of Kafka Partitions. Good to use Instance group to resize.","title":"Dmux Components"},{"location":"Architecture/#dmuxconnection-block-digram","text":"DmuxConnection connects Source to Sink. Example connectionTypes = kafka_http, kafka_foxtrot. DmuxConnection enables you to configure dmuxSize concurrent consumer Sinks along with batching to achieve high Throughput. Dmux is written Go and leverages go coroutines and channels to achieve high vertical scale. Most cases would need just one Dmux instance to run many DmuxConnections. KafkaSource interface in Dmux is a High Available KafkaConsumer which reuses the Zookeeper used by KafkaBrokers for Partition Balancing and Offset management. During Partition Rebalancing between Dmux instances, Client can expect replay but there will be no data loss. Dmux ensures ordering per Key when distributor = Hash. Note this is default distributor type.","title":"DmuxConnection Block Digram"},{"location":"Batching/","text":"Batching \u00b6 Batching capability enables you to specify batch_size . Batching logic is implemented differently in go-dmux to help achieve better performance. When using Modulo Hash Distributor, Batching is normally implemented by request collapsing per Consumer. Dmux implements batching by increasing number of channels = batchSize * size and having one consumer per batch of channels. Consumption inside a consumer is to pop the head from each channel serially and build batch request for sink. This ensures batch entities are not of same group (Hash group) and can be processed in parallel by sink. Given below is an illustration of the above idea. eg.. if Dmux size == 2 and batch_size == 2; source = [O1t1,O2t1,O1t2,O1t3,O3t1,O3t2,O2t2,O4t1,O5t1,O3t3] # O1 - Order1 and t1 = time 1 # hence O1t1 = Order 1 at time 1 -> Lets assume this is distributed using HashDistributor as: sink1 - [O1t1,O1t2,O1t3,O4t1,O5t1] sink2 - [O2t1,O3t1,O3t2,O2t2,O3t3] Now Traditional batching would batch this before calling Sink as sink1 - [[O1t1,O1,t2], [O1t3,O4,t1], [O5t1]] similar for sink2. The problem here is that when the App endpoint processing this HTTPSink call to it needs to process this, it has to process sequentially to ensure ordering is maintained. Imaging Batch 1 from Sink 1 = [O1t1, O1t2] if processed parallel has potential list of out of order in processing. Dmux implementation of Batching for same source : Dmux creates channels = batchsize * dmuxSize => 2* 2 = 4 -> Now the HashDistributor will distribute this to the 4 channels as channel1 = [O1t1,O1t2,O1t3] channel2 = [O2t1,O2t2] channel3 = [O3t1,O3t2,O3t3] channel4 = [O4t1,O5t1] Sink to channel mapping would be sink1 = [channel1, channel2] sink2 = [channel3, channel4] each sink ensures it batches by reading top entry from each of the channel. Hence batch will contain entries which are not of the same groupId. Enabling client o process them concurrently.","title":"Batching"},{"location":"Batching/#batching","text":"Batching capability enables you to specify batch_size . Batching logic is implemented differently in go-dmux to help achieve better performance. When using Modulo Hash Distributor, Batching is normally implemented by request collapsing per Consumer. Dmux implements batching by increasing number of channels = batchSize * size and having one consumer per batch of channels. Consumption inside a consumer is to pop the head from each channel serially and build batch request for sink. This ensures batch entities are not of same group (Hash group) and can be processed in parallel by sink. Given below is an illustration of the above idea. eg.. if Dmux size == 2 and batch_size == 2; source = [O1t1,O2t1,O1t2,O1t3,O3t1,O3t2,O2t2,O4t1,O5t1,O3t3] # O1 - Order1 and t1 = time 1 # hence O1t1 = Order 1 at time 1 -> Lets assume this is distributed using HashDistributor as: sink1 - [O1t1,O1t2,O1t3,O4t1,O5t1] sink2 - [O2t1,O3t1,O3t2,O2t2,O3t3] Now Traditional batching would batch this before calling Sink as sink1 - [[O1t1,O1,t2], [O1t3,O4,t1], [O5t1]] similar for sink2. The problem here is that when the App endpoint processing this HTTPSink call to it needs to process this, it has to process sequentially to ensure ordering is maintained. Imaging Batch 1 from Sink 1 = [O1t1, O1t2] if processed parallel has potential list of out of order in processing. Dmux implementation of Batching for same source : Dmux creates channels = batchsize * dmuxSize => 2* 2 = 4 -> Now the HashDistributor will distribute this to the 4 channels as channel1 = [O1t1,O1t2,O1t3] channel2 = [O2t1,O2t2] channel3 = [O3t1,O3t2,O3t3] channel4 = [O4t1,O5t1] Sink to channel mapping would be sink1 = [channel1, channel2] sink2 = [channel3, channel4] each sink ensures it batches by reading top entry from each of the channel. Hence batch will contain entries which are not of the same groupId. Enabling client o process them concurrently.","title":"Batching"},{"location":"Benefits/","text":"Benefits \u00b6 Tech Stack Consolidation (Better Reuse and Developer Productivity): \u00b6 Developers focus only on Building API's over Apps irrespective of Stream Processing or User request handling. No need to worry about new components such as Nimbus, Supervisor in case of Storm. Monitoring and Scaling are solved problems for HA Apps. Developers can reuse patterns of dispatch compose and state machine orchestration used in Apps. Single Code base for App and Storm makes it easier to maintain. Just add another API to build stream processing logic. App Developers do not need to worry about processing message in order. Dmux provides ordering per Key guarantees by ensuring the next call for the same key is only made when first call returns 2xx. Simpler Predictive Scalability: \u00b6 Simple way to scale with Dmux is by increasing fanout concurrency or increasing number of App boxes acting as HTTPSink or both depending on which is the bottleneck. For eg. Note: This is just a simple approach which works for most cases. If avg latency for processing a message is 500ms. (Very Slow processing use case) And Single App can process 200 concurrent request at 70% CPU saturation. Then we can get Throughput of 400 req/sec from a single box. Now we can meet given QPS needs by just adding the right number of boxes and increasing Dmux concurrency. For meeting 2000 QPS. - We need at least 5 App boxes behind the HTTPSink VIP. = 5 * 400 - We need increase fanout of size of Dmux = 1000 concurrent request = 5 * 200 * We can achieve Vertical Scalability in single Dmux instance by touching two properties in Dmux config, size and pending_acks . Increasing them till we see good utilization of VM Note: you are more likely to saturate SinkApp boxes before you saturate at a single Dmux instance We can achieve Horizontal Scalability by increasing number of boxes for Dmux or sinkApp boxes, based on which is saturated. Note: Dmux horizontal scalability limit is limited by number of Kafka Partitions in case of KafkaSource. It's unlikely you will hit this limit given Dmux's vertical scale. Update Batching is now added, which helps in better throughput. Easier to achieve Better Performance \u00b6 Dmux performance was much better than Storm-0.9.5. Our basic testing had go-dmux draining Kafka in 1/10 time of Storm-0.9.5. Storm-0.9.5 took close to 5hrs to drain the queue when go-dmux could do it in 30min. Note : This was with our default tuning of Storm-0.9.5. We suspect there was backpreassure being built up and our basic tuning with count of workers, executors, max_spout_pending did not help. We did not dig deeper into Storm to find the bottleneck and solve for it in this testing. Check Graph in TestSetup which displays Kafka Lag monitoring Graph that shows Drain rate of same Topology in Storm vs Dmux + HttpAPP end point when the KafkaConsumer when force restarted.","title":"Benefits"},{"location":"Benefits/#benefits","text":"","title":"Benefits"},{"location":"Benefits/#tech-stack-consolidation-better-reuse-and-developer-productivity","text":"Developers focus only on Building API's over Apps irrespective of Stream Processing or User request handling. No need to worry about new components such as Nimbus, Supervisor in case of Storm. Monitoring and Scaling are solved problems for HA Apps. Developers can reuse patterns of dispatch compose and state machine orchestration used in Apps. Single Code base for App and Storm makes it easier to maintain. Just add another API to build stream processing logic. App Developers do not need to worry about processing message in order. Dmux provides ordering per Key guarantees by ensuring the next call for the same key is only made when first call returns 2xx.","title":"Tech Stack Consolidation (Better Reuse and Developer Productivity):"},{"location":"Benefits/#simpler-predictive-scalability","text":"Simple way to scale with Dmux is by increasing fanout concurrency or increasing number of App boxes acting as HTTPSink or both depending on which is the bottleneck. For eg. Note: This is just a simple approach which works for most cases. If avg latency for processing a message is 500ms. (Very Slow processing use case) And Single App can process 200 concurrent request at 70% CPU saturation. Then we can get Throughput of 400 req/sec from a single box. Now we can meet given QPS needs by just adding the right number of boxes and increasing Dmux concurrency. For meeting 2000 QPS. - We need at least 5 App boxes behind the HTTPSink VIP. = 5 * 400 - We need increase fanout of size of Dmux = 1000 concurrent request = 5 * 200 * We can achieve Vertical Scalability in single Dmux instance by touching two properties in Dmux config, size and pending_acks . Increasing them till we see good utilization of VM Note: you are more likely to saturate SinkApp boxes before you saturate at a single Dmux instance We can achieve Horizontal Scalability by increasing number of boxes for Dmux or sinkApp boxes, based on which is saturated. Note: Dmux horizontal scalability limit is limited by number of Kafka Partitions in case of KafkaSource. It's unlikely you will hit this limit given Dmux's vertical scale. Update Batching is now added, which helps in better throughput.","title":"Simpler Predictive Scalability:"},{"location":"Benefits/#easier-to-achieve-better-performance","text":"Dmux performance was much better than Storm-0.9.5. Our basic testing had go-dmux draining Kafka in 1/10 time of Storm-0.9.5. Storm-0.9.5 took close to 5hrs to drain the queue when go-dmux could do it in 30min. Note : This was with our default tuning of Storm-0.9.5. We suspect there was backpreassure being built up and our basic tuning with count of workers, executors, max_spout_pending did not help. We did not dig deeper into Storm to find the bottleneck and solve for it in this testing. Check Graph in TestSetup which displays Kafka Lag monitoring Graph that shows Drain rate of same Topology in Storm vs Dmux + HttpAPP end point when the KafkaConsumer when force restarted.","title":"Easier to achieve Better Performance"},{"location":"FAQ/","text":"TODO \u00b6","title":"FAQ"},{"location":"FAQ/#todo","text":"","title":"TODO"},{"location":"Onboarding/","text":"About \u00b6 go-dmux packaging and usage in debian 9. go-dmux can be used to connect kafka source to http sink to provide seamless message processing and API calls to Application. For details about go-dmux check https://github.com/flipkart-incubator/pages/go-dmux Releases \u00b6 Date PackageVersion - RepoVersion Package Name Repo Name Comments Dependencies \u00b6 Dependency Repo Name","title":"Onboarding"},{"location":"Onboarding/#about","text":"go-dmux packaging and usage in debian 9. go-dmux can be used to connect kafka source to http sink to provide seamless message processing and API calls to Application. For details about go-dmux check https://github.com/flipkart-incubator/pages/go-dmux","title":"About"},{"location":"Onboarding/#releases","text":"Date PackageVersion - RepoVersion Package Name Repo Name Comments","title":"Releases"},{"location":"Onboarding/#dependencies","text":"Dependency Repo Name","title":"Dependencies"},{"location":"Sinks/","text":"Sinks as name implies are the destination for the messages. Source messages are demultiplexed based on Dmux size and sent to sink. Dmux size represent number of go routines running each Sink concurrently. Dmux batch_size could be used to batch messages when they hit sink. Dmux implementation of batching is different. Check Batching Tab for more details. HTTPSink \u00b6 This is a generic HTTPSink, that is built over go http lib. It provides constructs of pre and post hooks. This is implemented as infinite retry in case of failure. Once a message (or batch of messages) reach HTTPSink. HTTPSink will not take next message till this is processed.","title":"Sinks"},{"location":"Sinks/#httpsink","text":"This is a generic HTTPSink, that is built over go http lib. It provides constructs of pre and post hooks. This is implemented as infinite retry in case of failure. Once a message (or batch of messages) reach HTTPSink. HTTPSink will not take next message till this is processed.","title":"HTTPSink"},{"location":"Sources/","text":"Sources implement logic go generate infinite stream of messages. Sources also have capability of load balancing when go-dmux connection is running with multiple go-dmux instance to enable distribution of incoming stream between the multiple go-dmux instances running the same connection. KafaSource \u00b6 Kafka source is wrapper implementation over wvanbergen/kafka/consumergroup which uses sarama.kafka. sarama.kafka was chosen as it supported kafka 0.8 version. wvanbergen/kafka/consumergroup provide HA worker constructs to handle offset using zookeeper and load balance partition of kafka topic across go-dmux instances having the same connection. Kafka Consumer created by KafkaSource is similar to KafkaConsoleConsumer, it creates entries under consumer node of kafkaPath in zookeeper that holds the consumer offset. Lag in topic consumption can be monitored by checking offset diff between producer and consumer. For Transact team, a logic to collect this metric and push to JMX exist and can be configured. TODO - add notes on how to configure Other team can setup the daemon process to collect and push metric to JMX. Project with code to monitor Kafka offsets via Zookeeper: TODO - (move above project to transact-commons add documentation to help setup monitoring) Note: \u00b6 Always setup monitoring and track lag, You should also setup alerting on this lag. This is the first level indicator when things don't work","title":"Sources"},{"location":"Sources/#kafasource","text":"Kafka source is wrapper implementation over wvanbergen/kafka/consumergroup which uses sarama.kafka. sarama.kafka was chosen as it supported kafka 0.8 version. wvanbergen/kafka/consumergroup provide HA worker constructs to handle offset using zookeeper and load balance partition of kafka topic across go-dmux instances having the same connection. Kafka Consumer created by KafkaSource is similar to KafkaConsoleConsumer, it creates entries under consumer node of kafkaPath in zookeeper that holds the consumer offset. Lag in topic consumption can be monitored by checking offset diff between producer and consumer. For Transact team, a logic to collect this metric and push to JMX exist and can be configured. TODO - add notes on how to configure Other team can setup the daemon process to collect and push metric to JMX. Project with code to monitor Kafka offsets via Zookeeper: TODO - (move above project to transact-commons add documentation to help setup monitoring)","title":"KafaSource"},{"location":"Sources/#note","text":"Always setup monitoring and track lag, You should also setup alerting on this lag. This is the first level indicator when things don't work","title":"Note:"},{"location":"Tech/","text":"Tech Stack \u00b6 go1.12 - golang version Shopify/sarama - sarma kafka library for kafka servers < 1.0 version wvanbergen/kafka - HA kafka consumer (partition rebalancing etcc.) using zookeeper natefinch/lumberjack.v2 - Log rotation mod - mod for dependency management Package Info \u00b6 \u251c\u2500\u2500 Gopkg.lock \u251c\u2500\u2500 Gopkg.toml \u251c\u2500\u2500 README.md \u251c\u2500\u2500 build.sh \u251c\u2500\u2500 conf.json \u251c\u2500\u2500 config.go #config integration to read config for dmux conneciton. \u251c\u2500\u2500 connection #connections \u2502 \u251c\u2500\u2500 kafka_foxtrot_conn.go \u2502 \u2514\u2500\u2500 kafka_http_conn.go \u251c\u2500\u2500 controller.go \u251c\u2500\u2500 core #core dmux logic \u2502 \u251c\u2500\u2500 distribute.go \u2502 \u251c\u2500\u2500 distribute_test.go \u2502 \u251c\u2500\u2500 dmux.go \u2502 \u251c\u2500\u2500 dmux_test.go \u2502 \u251c\u2500\u2500 util.go \u2502 \u2514\u2500\u2500 util_test.go \u251c\u2500\u2500 default.log \u251c\u2500\u2500 docs \u2502 \u251c\u2500\u2500 Architecture.md \u2502 \u251c\u2500\u2500 Batching.md \u2502 \u251c\u2500\u2500 Benefits.md \u2502 \u251c\u2500\u2500 Onboarding.md \u2502 \u251c\u2500\u2500 Tech.md \u2502 \u251c\u2500\u2500 Test_Setup.md \u2502 \u251c\u2500\u2500 config.md \u2502 \u251c\u2500\u2500 connections.md \u2502 \u251c\u2500\u2500 deployment.md \u2502 \u251c\u2500\u2500 img \u2502 \u251c\u2500\u2500 index.md \u2502 \u251c\u2500\u2500 limitations.md \u2502 \u2514\u2500\u2500 monitoring.md \u251c\u2500\u2500 http #http logic \u2502 \u251c\u2500\u2500 http_sink.go \u2502 \u251c\u2500\u2500 http_sink_test.go \u2502 \u251c\u2500\u2500 sample_order_ids.txt \u2502 \u2514\u2500\u2500 sink_test.json \u251c\u2500\u2500 images \u2502 \u251c\u2500\u2500 go_dmux.jpg \u2502 \u251c\u2500\u2500 go_dmux_component.png \u2502 \u2514\u2500\u2500 kafka_lag_dmux_vs_storm.png \u251c\u2500\u2500 java #java client tools to consume \u2502 \u2514\u2500\u2500 godmux-tools \u251c\u2500\u2500 kafka #kafa client <1.0 logic \u2502 \u251c\u2500\u2500 kafka_source.go \u2502 \u251c\u2500\u2500 kafka_source_test.go \u2502 \u2514\u2500\u2500 offset_tracker.go \u251c\u2500\u2500 logging #logging code \u2502 \u2514\u2500\u2500 logging.go \u251c\u2500\u2500 main.go #bootstrap main function \u251c\u2500\u2500 mkdocs.yml \u2514\u2500\u2500 vendor \u251c\u2500\u2500 github.com \u2514\u2500\u2500 gopkg.in","title":"Tech"},{"location":"Tech/#tech-stack","text":"go1.12 - golang version Shopify/sarama - sarma kafka library for kafka servers < 1.0 version wvanbergen/kafka - HA kafka consumer (partition rebalancing etcc.) using zookeeper natefinch/lumberjack.v2 - Log rotation mod - mod for dependency management","title":"Tech Stack"},{"location":"Tech/#package-info","text":"\u251c\u2500\u2500 Gopkg.lock \u251c\u2500\u2500 Gopkg.toml \u251c\u2500\u2500 README.md \u251c\u2500\u2500 build.sh \u251c\u2500\u2500 conf.json \u251c\u2500\u2500 config.go #config integration to read config for dmux conneciton. \u251c\u2500\u2500 connection #connections \u2502 \u251c\u2500\u2500 kafka_foxtrot_conn.go \u2502 \u2514\u2500\u2500 kafka_http_conn.go \u251c\u2500\u2500 controller.go \u251c\u2500\u2500 core #core dmux logic \u2502 \u251c\u2500\u2500 distribute.go \u2502 \u251c\u2500\u2500 distribute_test.go \u2502 \u251c\u2500\u2500 dmux.go \u2502 \u251c\u2500\u2500 dmux_test.go \u2502 \u251c\u2500\u2500 util.go \u2502 \u2514\u2500\u2500 util_test.go \u251c\u2500\u2500 default.log \u251c\u2500\u2500 docs \u2502 \u251c\u2500\u2500 Architecture.md \u2502 \u251c\u2500\u2500 Batching.md \u2502 \u251c\u2500\u2500 Benefits.md \u2502 \u251c\u2500\u2500 Onboarding.md \u2502 \u251c\u2500\u2500 Tech.md \u2502 \u251c\u2500\u2500 Test_Setup.md \u2502 \u251c\u2500\u2500 config.md \u2502 \u251c\u2500\u2500 connections.md \u2502 \u251c\u2500\u2500 deployment.md \u2502 \u251c\u2500\u2500 img \u2502 \u251c\u2500\u2500 index.md \u2502 \u251c\u2500\u2500 limitations.md \u2502 \u2514\u2500\u2500 monitoring.md \u251c\u2500\u2500 http #http logic \u2502 \u251c\u2500\u2500 http_sink.go \u2502 \u251c\u2500\u2500 http_sink_test.go \u2502 \u251c\u2500\u2500 sample_order_ids.txt \u2502 \u2514\u2500\u2500 sink_test.json \u251c\u2500\u2500 images \u2502 \u251c\u2500\u2500 go_dmux.jpg \u2502 \u251c\u2500\u2500 go_dmux_component.png \u2502 \u2514\u2500\u2500 kafka_lag_dmux_vs_storm.png \u251c\u2500\u2500 java #java client tools to consume \u2502 \u2514\u2500\u2500 godmux-tools \u251c\u2500\u2500 kafka #kafa client <1.0 logic \u2502 \u251c\u2500\u2500 kafka_source.go \u2502 \u251c\u2500\u2500 kafka_source_test.go \u2502 \u2514\u2500\u2500 offset_tracker.go \u251c\u2500\u2500 logging #logging code \u2502 \u2514\u2500\u2500 logging.go \u251c\u2500\u2500 main.go #bootstrap main function \u251c\u2500\u2500 mkdocs.yml \u2514\u2500\u2500 vendor \u251c\u2500\u2500 github.com \u2514\u2500\u2500 gopkg.in","title":"Package Info"},{"location":"Test_Setup/","text":"Test Setup: \u00b6 Number of Storm Workers == Number of App Boxes behind VIP == Partitions of this Kafka topic == 8. AppBoxes running Dropwizard with 256 worker threads. The boxes were behind VIP which go-dmux was pointing to. Single go-dmux instance with size = 2000, pending_acks = 50000. Strom worker tried with max_spout_pending = 100000, and executors count = 256, 512, 1024. Logic in App == Logic in Spout + Bolt. Both storm and dmux where run with force_restart = true. We tried restarting Storm with different configuration to enable better performance after go-dmux drained. Hence multiple peaks are present in the graph. We believe better Performance of Dmux is because: \u00b6 Dmux avoids cross JVM chatter for single request processing. In Storm there are multiple queue hops in Spout -> Bolt. Dmux does not have any single choke point for its processing. In Storm Zk can be bottleneck as you increase number of executors and workers. Dmux has much simpler offset tracking which runs within single Dmux routine for the source partitions it owns. There is not cross process chatter for offset tracking. In Strom cross chatter leads to un-predictable performance when increasing max_spout_pending.","title":"Test Setup"},{"location":"Test_Setup/#test-setup","text":"Number of Storm Workers == Number of App Boxes behind VIP == Partitions of this Kafka topic == 8. AppBoxes running Dropwizard with 256 worker threads. The boxes were behind VIP which go-dmux was pointing to. Single go-dmux instance with size = 2000, pending_acks = 50000. Strom worker tried with max_spout_pending = 100000, and executors count = 256, 512, 1024. Logic in App == Logic in Spout + Bolt. Both storm and dmux where run with force_restart = true. We tried restarting Storm with different configuration to enable better performance after go-dmux drained. Hence multiple peaks are present in the graph.","title":"Test Setup:"},{"location":"Test_Setup/#we-believe-better-performance-of-dmux-is-because","text":"Dmux avoids cross JVM chatter for single request processing. In Storm there are multiple queue hops in Spout -> Bolt. Dmux does not have any single choke point for its processing. In Storm Zk can be bottleneck as you increase number of executors and workers. Dmux has much simpler offset tracking which runs within single Dmux routine for the source partitions it owns. There is not cross process chatter for offset tracking. In Strom cross chatter leads to un-predictable performance when increasing max_spout_pending.","title":"We believe better Performance of Dmux is because:"},{"location":"config/","text":"Config Details \u00b6 DMux reads basic config during start-up from conf.json. It has list of dmuxItems. On startup DMux reads at run time the config for each dmuxItems to create and start connection. Basic config include: * name * dmuxItems * logging Sample config file which which DMux will reading during start-up { \"name\" : \"myDmux\" , \"dmuxItems\" : [ { \"name\" : \"sample_name\" , \"disabled\" : false , \"connectionType\" : \"kafka_http\" , \"connection\" : { \"dmux\" : { \"size\" : 250 , \"distributor_type\" : \"Hash\" , \"batch_size\" : 1 }, \"source\" : { \"name\" : \"source_name\" , \"zk_path\" : \"zookeeper1:2181,zookeeper2:2181,zookeeper3:2181/zk-path\" , \"kafka_version_major\" : 2 , \"topic\" : \"source_topic\" , \"force_restart\" : false , \"read_newest\" : true }, \"pending_acks\" : 1000000 , \"sink\" : { \"endpoint\" : \"http://elb/1.0/api/consume\" , \"timeout\" : \"10s\" , \"retry_interval\" : \"100ms\" , \"headers\" : [ { \"name\" : \"X-Client\" , \"value\" : \"go-dumx\" }, { \"name\" : \"Content-Type\" , \"value\" : \"application/json\" } ], \"method\" : \"POST\" } } } ], \"logging\" : { \"path\" : \"default.log\" , \"enable_debug\" : false , \"rotation\" : { \"size_in_mb\" : 256 , \"retention_count\" : 5 , \"retention_days\" : 90 , \"compress\" : true } } } Config Details: \u00b6 Config Key Default Comment name NA The name given for this dmux instance dmuxItems NA dmuxItems are dmuxConnections each connection has name and connectionType - name is used to refer to its config and connectionType can be kafka_http or kafka_foxtrot dmux.size 10 demultiplex size. If size = 10; 1 Source will connect to 10 sink. Use this to increase throughput until the client box resource is saturated. dmux.distributor_type Hash Type of distributor other option is RoundRobin dmux.batch_size 1 make this value > 1 to specify batching source.name NA consumer_group_name for Kafka consumer. This will be used in zookeeper offset tracking source.zk_path NA kafka zookeeper path source.topic NA kafka topic you want to consume source.force_restart false set to true to reset consumer to consume from start source.read_newest false read from head if this value is set, this config will take in effect only if force_restart is true source.kafka_version_major int set to 2 if the source is a kafka 2.x.x cluster, 1 if the source is a kafka 1.x.x cluster otherwise ignore it for default (0.8.2) sink.endpoint NA http endpoint to hit, If connectionType == kafka_http then url given here will be appended by /{topic}/{partition}/{key}/{offset}. This will be POST call with byte[] in body, if connectionType == kafka_foxtrot then expected url should be http://foxtrot.com:10000/foxtrot/v1/document/ KEY_NAME where KEY_NAME is replaced by kafka-key and body will be JSON. Note: if batch_size is > 1 then batching will result in byte[][] payload for kafka_http connection and []json payload for foxtrot connection sink.timeout 10s http roundtrip timeout sink.retry_interval 100ms time interval to sleep before retry if http call failed. Note: go-dmux has no concept of sideline, It will do infinite retries. Client is expected to build sideline if need at the Sink Application being hit sink.headers NA static headers to be added in http call. Note: Content-Type:application/octet-stream will be added for POST calls for kafka_http and application/json for kafka_foxtrot pending_acks 10000 No of unordered acks acceptable till go-dmux starts to apply backpressure to the source. Increase this if QPS does not increase on increasing size and you can see Warning Log in go-dmux that you hit this threshold. Cost of increasing this is memory and larger no of records replay when go-dmux crashes. logging.path /var/log/go-dmux/default.log log file path dmuxLogEnableDebug false boolean flag to enable debug logging logging.rotation.size_in_mb 256 log size logging.rotation.retention_count 5 number of log files to keep, rest are archived logging.rotation.retention_days 90 number of days to keep log files, rest are archive. logging.rotation.compress true will compress log files which are rotated Note which every condition becomes true first in retention_count and retention_days will apply.","title":"Config"},{"location":"config/#config-details","text":"DMux reads basic config during start-up from conf.json. It has list of dmuxItems. On startup DMux reads at run time the config for each dmuxItems to create and start connection. Basic config include: * name * dmuxItems * logging Sample config file which which DMux will reading during start-up { \"name\" : \"myDmux\" , \"dmuxItems\" : [ { \"name\" : \"sample_name\" , \"disabled\" : false , \"connectionType\" : \"kafka_http\" , \"connection\" : { \"dmux\" : { \"size\" : 250 , \"distributor_type\" : \"Hash\" , \"batch_size\" : 1 }, \"source\" : { \"name\" : \"source_name\" , \"zk_path\" : \"zookeeper1:2181,zookeeper2:2181,zookeeper3:2181/zk-path\" , \"kafka_version_major\" : 2 , \"topic\" : \"source_topic\" , \"force_restart\" : false , \"read_newest\" : true }, \"pending_acks\" : 1000000 , \"sink\" : { \"endpoint\" : \"http://elb/1.0/api/consume\" , \"timeout\" : \"10s\" , \"retry_interval\" : \"100ms\" , \"headers\" : [ { \"name\" : \"X-Client\" , \"value\" : \"go-dumx\" }, { \"name\" : \"Content-Type\" , \"value\" : \"application/json\" } ], \"method\" : \"POST\" } } } ], \"logging\" : { \"path\" : \"default.log\" , \"enable_debug\" : false , \"rotation\" : { \"size_in_mb\" : 256 , \"retention_count\" : 5 , \"retention_days\" : 90 , \"compress\" : true } } }","title":"Config Details"},{"location":"config/#config-details_1","text":"Config Key Default Comment name NA The name given for this dmux instance dmuxItems NA dmuxItems are dmuxConnections each connection has name and connectionType - name is used to refer to its config and connectionType can be kafka_http or kafka_foxtrot dmux.size 10 demultiplex size. If size = 10; 1 Source will connect to 10 sink. Use this to increase throughput until the client box resource is saturated. dmux.distributor_type Hash Type of distributor other option is RoundRobin dmux.batch_size 1 make this value > 1 to specify batching source.name NA consumer_group_name for Kafka consumer. This will be used in zookeeper offset tracking source.zk_path NA kafka zookeeper path source.topic NA kafka topic you want to consume source.force_restart false set to true to reset consumer to consume from start source.read_newest false read from head if this value is set, this config will take in effect only if force_restart is true source.kafka_version_major int set to 2 if the source is a kafka 2.x.x cluster, 1 if the source is a kafka 1.x.x cluster otherwise ignore it for default (0.8.2) sink.endpoint NA http endpoint to hit, If connectionType == kafka_http then url given here will be appended by /{topic}/{partition}/{key}/{offset}. This will be POST call with byte[] in body, if connectionType == kafka_foxtrot then expected url should be http://foxtrot.com:10000/foxtrot/v1/document/ KEY_NAME where KEY_NAME is replaced by kafka-key and body will be JSON. Note: if batch_size is > 1 then batching will result in byte[][] payload for kafka_http connection and []json payload for foxtrot connection sink.timeout 10s http roundtrip timeout sink.retry_interval 100ms time interval to sleep before retry if http call failed. Note: go-dmux has no concept of sideline, It will do infinite retries. Client is expected to build sideline if need at the Sink Application being hit sink.headers NA static headers to be added in http call. Note: Content-Type:application/octet-stream will be added for POST calls for kafka_http and application/json for kafka_foxtrot pending_acks 10000 No of unordered acks acceptable till go-dmux starts to apply backpressure to the source. Increase this if QPS does not increase on increasing size and you can see Warning Log in go-dmux that you hit this threshold. Cost of increasing this is memory and larger no of records replay when go-dmux crashes. logging.path /var/log/go-dmux/default.log log file path dmuxLogEnableDebug false boolean flag to enable debug logging logging.rotation.size_in_mb 256 log size logging.rotation.retention_count 5 number of log files to keep, rest are archived logging.rotation.retention_days 90 number of days to keep log files, rest are archive. logging.rotation.compress true will compress log files which are rotated Note which every condition becomes true first in retention_count and retention_days will apply.","title":"Config Details:"},{"location":"connections/","text":"Dmux Connections define Source to Sink connection. One Dmux instance can run multiple dmux connections. Configuration options change per connection based on nature of Source and Sink. Logging is a common configuration option across all connections. The following Connections are supported: kafka_http \u00b6 This the most simplest connection that connects KafkaSource to HttpSink. It expects KafkaSource to have KeydMessage - (Key + Value). ModuloHash will happen on Key if distributor type is Hash (default). HttpSink will create the following url : http://endpoint/{topic}/{partition}/{key}/{offset}. The payload will be byte[] value from Kafka value. Note Dmux does no processing on value. Any non 2xx response code will be considered as failure by HTTPSink and retried. Currently retry policy is fixed delay retry. You can configure retry_interval using config. If batch_size is specified for batching the URL will change to: http://endpoint/{topic}?batch=(partition1,key1,offset1~partition2,key2,offset2..) The query param batch was added to help make debugging easier. Batching in Dmux is implemented differently, To understand check Batching Tab. Payload in batch is encoded to convert 2d byte[] to 1d byte[], without any serialization. Encoding Format \u00b6 TODO Java Lib \u00b6 Java library exist to help decode : https://github.com/flipkart-incubator/go-dmux/tree/master/java/godmux-tools You can add the below POM dependency. (TODO) move this to release version <dependency> <groupId>com.flipkart.godmux.tools</groupId> <artifactId>godmux-tools</artifactId> <version>1.0-SNAPSHOT</version> </dependency> kafka_foxtrot \u00b6 This is an extension of kafka_http connection with customization specific to ingest to foxtrot. https://github.com/Flipkart/foxtrot Across transact we use foxtrot for monitoring and debugging and have client lib which write to Kafka in a specific message format needed by Foxtrot. TODO Add link to Foxtrot ingestion lib used in transact This connection works on this structure of Kafka message where key = tableName and value is foxtrot payload. Check https://github.com/Flipkart/foxtrot/wiki/Event-Ingestion to see foxtrot ingestion API. URL expected to be configured http://foxtrot.com:10000/foxtrot/v1/document/ KEY_NAME KEY_NAME will be replaced by tableName. batching is also supported and customised to format needed by foxtrot ingestion api. query params are added to both Single URL and Batch URL which represent topicName, offset, partition to simplify debugging.","title":"Connections"},{"location":"connections/#kafka_http","text":"This the most simplest connection that connects KafkaSource to HttpSink. It expects KafkaSource to have KeydMessage - (Key + Value). ModuloHash will happen on Key if distributor type is Hash (default). HttpSink will create the following url : http://endpoint/{topic}/{partition}/{key}/{offset}. The payload will be byte[] value from Kafka value. Note Dmux does no processing on value. Any non 2xx response code will be considered as failure by HTTPSink and retried. Currently retry policy is fixed delay retry. You can configure retry_interval using config. If batch_size is specified for batching the URL will change to: http://endpoint/{topic}?batch=(partition1,key1,offset1~partition2,key2,offset2..) The query param batch was added to help make debugging easier. Batching in Dmux is implemented differently, To understand check Batching Tab. Payload in batch is encoded to convert 2d byte[] to 1d byte[], without any serialization.","title":"kafka_http"},{"location":"connections/#encoding-format","text":"TODO","title":"Encoding Format"},{"location":"connections/#java-lib","text":"Java library exist to help decode : https://github.com/flipkart-incubator/go-dmux/tree/master/java/godmux-tools You can add the below POM dependency. (TODO) move this to release version <dependency> <groupId>com.flipkart.godmux.tools</groupId> <artifactId>godmux-tools</artifactId> <version>1.0-SNAPSHOT</version> </dependency>","title":"Java Lib"},{"location":"connections/#kafka_foxtrot","text":"This is an extension of kafka_http connection with customization specific to ingest to foxtrot. https://github.com/Flipkart/foxtrot Across transact we use foxtrot for monitoring and debugging and have client lib which write to Kafka in a specific message format needed by Foxtrot. TODO Add link to Foxtrot ingestion lib used in transact This connection works on this structure of Kafka message where key = tableName and value is foxtrot payload. Check https://github.com/Flipkart/foxtrot/wiki/Event-Ingestion to see foxtrot ingestion API. URL expected to be configured http://foxtrot.com:10000/foxtrot/v1/document/ KEY_NAME KEY_NAME will be replaced by tableName. batching is also supported and customised to format needed by foxtrot ingestion api. query params are added to both Single URL and Batch URL which represent topicName, offset, partition to simplify debugging.","title":"kafka_foxtrot"},{"location":"deployment/","text":"Packaging and Release Details: \u00b6 Usage \u00b6 To Run on Local \u00b6 ```sh Install go https://golang.org/doc/install or brew install golang) \u00b6 Clone go-dmux (git clone https://github.com/flipkart-incubator/go-dmux.git) in this folder. ~/go/src/github.com/go-dmux In IDE set GOROOT as the go directory path (generally it is /usr/local/go) and GOPATH as src directory where go-dmux project is there cd ~/$GOPATH/src mkdir github.com Build and Run \u00b6 cd go-dmux vim conf.json (update config as per your need) select go build in IDE and run as Project, set package path as github.com/flipkart-incubator/go-dmux, working directory as go-dmux project path, programme argument as conf.json and module as go-dmux","title":"Deployment"},{"location":"deployment/#packaging-and-release-details","text":"","title":"Packaging and Release Details:"},{"location":"deployment/#usage","text":"","title":"Usage"},{"location":"deployment/#to-run-on-local","text":"```sh","title":"To Run on Local"},{"location":"deployment/#install-go-httpsgolangorgdocinstall-or-brew-install-golang","text":"Clone go-dmux (git clone https://github.com/flipkart-incubator/go-dmux.git) in this folder. ~/go/src/github.com/go-dmux In IDE set GOROOT as the go directory path (generally it is /usr/local/go) and GOPATH as src directory where go-dmux project is there cd ~/$GOPATH/src mkdir github.com","title":"Install go https://golang.org/doc/install or brew install golang)"},{"location":"deployment/#build-and-run","text":"cd go-dmux vim conf.json (update config as per your need) select go build in IDE and run as Project, set package path as github.com/flipkart-incubator/go-dmux, working directory as go-dmux project path, programme argument as conf.json and module as go-dmux","title":"Build and Run"},{"location":"limitations/","text":"Drawbacks: \u00b6 ~~Dmux does not support batching.~~ Batching support has now been added. Dmux is stateless and does not support constructs that need persistance such as windowing, aggregations (time or pivot based) and sideline. The expectation is for the app to build such constructs using persistance. ( Note HttpSink call fails will infinitely retries and producer will get choked as pending_acks threshold would be hit). Very Low latent processing, using local rocksDB per processing to avoid Network overhead can not be achieved with go-dmux. Go-dmux helps in predictive scaling for throughput. Low latency is for App API performance tuning to solve for, but will use a DB isolated over network. ~~Sideline (Deprecated now, needs rethinking)~~ \u00b6 ~~go-dmux is stateless it requires client to manage persistance for sideline support. If sideline is not configured, go-dmux retries infinitely to execute the httpSink api endpoint till it gets at 2xx response code.~~ ~~ Note : Cost of enabling sideline will be 2x load to client boxes. Clients can avoid enabling this feature and alternative build sideline logic inside their application while processing request and provide 2xx response for performance.~~ ~~To enable sideline you just need to add new sink config \"sideline_after\"~~ sideline_after = 10 // this implies after 10 retries the message will get sidelined ~~Sideline works with 2 constructs, which Client must expose via API.~~ ~~ShouldTheMessageKeyBeSidelined:~~ ~~This is a GET call that is made to the same endpoint in the same url format as POST call to execute ( Note the url is the same url for single message i.e. http://host:port/path/topicName/partition/key/offset). This call is made before every httpSink POST call to validate if the given key needs to be prevented from processing and proactively sidelined as the same key of this message with prior offset would have been already and we want to sideline this as well to ensure ordering.~~ ~~The API works out of client response codes:~~ * ~~200 -> means the key should be proactively sidelined and prevented from processing~~ * ~~204 -> means the key can be allowed to process.~~ ~~ Note All other status codes result in retry to this GET call infinitely and continuous failure will apply back-pressure on the Source to stall processing.~~ ~~WriteToSideline:~~ ~~This is a PUT call that is made to the same endpoint in the same url format as POST call to execute ( Note the url is the same url for single message i.e. http://host:port/path/topicName/partition/key/offset). This call is made post httpSink POST call fails, to sideline the messages. Any Non 2xx response is considered failure~~ ~~ Note This PUT call will retry infinitely and continuous failure will apply back-presssure on the Source to stall processing.~~","title":"Limitations"},{"location":"limitations/#drawbacks","text":"~~Dmux does not support batching.~~ Batching support has now been added. Dmux is stateless and does not support constructs that need persistance such as windowing, aggregations (time or pivot based) and sideline. The expectation is for the app to build such constructs using persistance. ( Note HttpSink call fails will infinitely retries and producer will get choked as pending_acks threshold would be hit). Very Low latent processing, using local rocksDB per processing to avoid Network overhead can not be achieved with go-dmux. Go-dmux helps in predictive scaling for throughput. Low latency is for App API performance tuning to solve for, but will use a DB isolated over network.","title":"Drawbacks:"},{"location":"limitations/#sideline-deprecated-now-needs-rethinking","text":"~~go-dmux is stateless it requires client to manage persistance for sideline support. If sideline is not configured, go-dmux retries infinitely to execute the httpSink api endpoint till it gets at 2xx response code.~~ ~~ Note : Cost of enabling sideline will be 2x load to client boxes. Clients can avoid enabling this feature and alternative build sideline logic inside their application while processing request and provide 2xx response for performance.~~ ~~To enable sideline you just need to add new sink config \"sideline_after\"~~ sideline_after = 10 // this implies after 10 retries the message will get sidelined ~~Sideline works with 2 constructs, which Client must expose via API.~~ ~~ShouldTheMessageKeyBeSidelined:~~ ~~This is a GET call that is made to the same endpoint in the same url format as POST call to execute ( Note the url is the same url for single message i.e. http://host:port/path/topicName/partition/key/offset). This call is made before every httpSink POST call to validate if the given key needs to be prevented from processing and proactively sidelined as the same key of this message with prior offset would have been already and we want to sideline this as well to ensure ordering.~~ ~~The API works out of client response codes:~~ * ~~200 -> means the key should be proactively sidelined and prevented from processing~~ * ~~204 -> means the key can be allowed to process.~~ ~~ Note All other status codes result in retry to this GET call infinitely and continuous failure will apply back-pressure on the Source to stall processing.~~ ~~WriteToSideline:~~ ~~This is a PUT call that is made to the same endpoint in the same url format as POST call to execute ( Note the url is the same url for single message i.e. http://host:port/path/topicName/partition/key/offset). This call is made post httpSink POST call fails, to sideline the messages. Any Non 2xx response is considered failure~~ ~~ Note This PUT call will retry infinitely and continuous failure will apply back-presssure on the Source to stall processing.~~","title":"~~Sideline (Deprecated now, needs rethinking)~~"},{"location":"monitoring/","text":"Kafka Lag monitoring \u00b6 Current connections of go-dmux connect kafka source to http/foxtrot sink. kafka_source creates consumer offset entries in zookeeper similar to simple consumer. Hence you can reuse kafka_lag monitoring setup that is in place and place alerting on top of the lag Dashboards \u00b6 TODO - add scripted dashboards Alerts \u00b6 TODO- Users will need to setup alerts via scripts defined here Playbooks \u00b6 TODO","title":"Monitoring"},{"location":"monitoring/#kafka-lag-monitoring","text":"Current connections of go-dmux connect kafka source to http/foxtrot sink. kafka_source creates consumer offset entries in zookeeper similar to simple consumer. Hence you can reuse kafka_lag monitoring setup that is in place and place alerting on top of the lag","title":"Kafka Lag monitoring"},{"location":"monitoring/#dashboards","text":"TODO - add scripted dashboards","title":"Dashboards"},{"location":"monitoring/#alerts","text":"TODO- Users will need to setup alerts via scripts defined here","title":"Alerts"},{"location":"monitoring/#playbooks","text":"TODO","title":"Playbooks"}]}